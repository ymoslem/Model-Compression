{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e59fe6",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers huggingface_hub[hf_xet] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe5f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_lang = \"de\"\n",
    "# tgt_lang = \"zh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_cache_dir = \"/workspace/model/\"\n",
    "\n",
    "model_name = \"Qwen/Qwen2-Audio-7B-Instruct\"  # better use a fine-tuned model\n",
    "\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(model_name,\n",
    "                                                           torch_dtype=torch.bfloat16,\n",
    "                                                           cache_dir=model_cache_dir,\n",
    "                                                          ).to(\"cuda\").eval()\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "print(\"Model loaded:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa18e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 8,397,094,912\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "model_parameters = count_parameters(model)\n",
    "print(f\"Model parameters: {model_parameters:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aca982",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_state_dict = model.state_dict()\n",
    "print(f\"Number of keys in the original state_dict: {len(original_state_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba270ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.audio_tower.layers), len(model.language_model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500b352d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2AudioEncoderConfig {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"d_model\": 1280,\n",
       "  \"dropout\": 0.0,\n",
       "  \"encoder_attention_heads\": 20,\n",
       "  \"encoder_ffn_dim\": 5120,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 32,\n",
       "  \"init_std\": 0.02,\n",
       "  \"max_source_positions\": 1500,\n",
       "  \"model_type\": \"qwen2_audio_encoder\",\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_mel_bins\": 128,\n",
       "  \"scale_embedding\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.51.3\"\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.audio_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df14ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"eos_token_id\": 151645,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"max_window_layers\": 28,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000,\n",
       "  \"sliding_window\": 32768,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_mrope\": false,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 156032\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.text_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737e465",
   "metadata": {},
   "source": [
    "# Downscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_remove = sorted([13, 3, 20, 9, 29, 1, 19, 27])  # change based on layer importance evaluation\n",
    "layers_to_keep = [i for i in range(32) if i not in layers_to_remove]\n",
    "\n",
    "print(f\"Pruning: {len(layers_to_keep)} layers:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4f8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the config...\n",
      "Current values: 32 32 32\n",
      "Model parameters: 6,777,929,728\n",
      "New values: 32 32 24\n"
     ]
    }
   ],
   "source": [
    "# Downscaling -- pruning decoder layers only (recommended; see the paper)\n",
    "# For pruning the encoder too, uncomment the lines marked as \"encoder\"...\n",
    "# ... in which case it needs to be part of the layer importance evaluation.\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# audio_layers = model.audio_tower.layers  # encoder\n",
    "lm_layers = model.language_model.model.layers  # decoder\n",
    "\n",
    "# model.audio_tower.layers = nn.ModuleList([audio_layers[n] for n in layers_to_keep_enc])  # encoder\n",
    "model.language_model.model.layers = nn.ModuleList([lm_layers[n] for n in layers_to_keep])  # decoder\n",
    "\n",
    "\n",
    "# Ensure the config reflects the actual number of layers\n",
    "print(\"Updating the config...\")\n",
    "print(\"Current values:\",\n",
    "      model.config.audio_config.encoder_layers,\n",
    "      model.config.audio_config.num_hidden_layers,\n",
    "      model.language_model.model.config.num_hidden_layers\n",
    "      )\n",
    "# model.config.audio_config.encoder_layers = len(model.audio_tower.layers)  # encoder\n",
    "# model.config.audio_config.num_hidden_layers = len(model.audio_tower.layers)  # encoder\n",
    "\n",
    "model.config.text_config.num_hidden_layers = len(model.language_model.model.layers)  # decoder\n",
    "model.language_model.model.config.num_hidden_layers = len(model.language_model.model.layers)  # decoder\n",
    "\n",
    "\n",
    "# Check the new number of parameters\n",
    "model_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {model_parameters:,}\")\n",
    "\n",
    "# Check the new number of layers\n",
    "print(\"New values:\" ,\n",
    "      model.config.audio_config.encoder_layers,\n",
    "      model.config.audio_config.num_hidden_layers,\n",
    "      model.language_model.model.config.num_hidden_layers\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd31076",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf03d1ed",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39ae981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Check if both configs have the same values\n",
    "print(model.config.text_config.num_hidden_layers)\n",
    "print(model.language_model.model.config.num_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After pruning the model\n",
    "# Examine the state dictionary keys\n",
    "state_dict = model.state_dict()\n",
    "print(f\"Number of keys in the original state_dict: {len(original_state_dict.keys())}\")\n",
    "print(f\"Number of keys in the new state_dict: {len(state_dict.keys())}\")\n",
    "\n",
    "# Check for any keys that might reference pruned layers\n",
    "audio_layer_keys = [k for k in state_dict.keys() if 'audio_tower.layers' in k]\n",
    "lm_layer_keys = [k for k in state_dict.keys() if 'language_model.model.layers' in k]\n",
    "\n",
    "# Print the unique layer indices in the state dict\n",
    "audio_indices = set([int(k.split('.')[2]) for k in audio_layer_keys])\n",
    "lm_indices = set([int(k.split('.')[3]) for k in lm_layer_keys])\n",
    "\n",
    "print(f\"Audio layer indices in state_dict: {sorted(audio_indices)}\")\n",
    "print(f\"LM layer indices in state_dict: {sorted(lm_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Save the pruned model locally (for verification purposes)\n",
    "# pruned_model_path = f\"pruned_qwen2_audio_ft_model_b16_{tgt_lang}\"\n",
    "# model.save_pretrained(pruned_model_path)\n",
    "# processor.save_pretrained(pruned_model_path)\n",
    "\n",
    "# !ls -lh {pruned_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb5f5e",
   "metadata": {},
   "source": [
    "# Upload to the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ead10",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers_e = len(model.audio_tower.layers)\n",
    "num_layers_d = len(model.language_model.model.layers)\n",
    "\n",
    "user_id = \"ymoslem\"  # change to your user ID\n",
    "output_model = f\"ymoslem/qwen-audio-en-{tgt_lang}-{num_layers_e}-{num_layers_d}layers\"\n",
    "output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the new model to Hugging Face\n",
    "\n",
    "model.push_to_hub(output_model,\n",
    "                  private=True,\n",
    "                  )\n",
    "\n",
    "processor.push_to_hub(output_model,\n",
    "                      private=True,\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
