{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flOEsz4Y3Qus"
   },
   "source": [
    "# Speech Translation (Qwen-Audio)\n",
    "* [Qwen/Qwen2-Audio-7B-Instruct](https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55fb8d21-df06-472a-99dd-b59567be6dad"
   },
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "95048026-a3b7-43f0-a274-1bad65e407b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 24 22:09:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H200                    On  |   00000000:AA:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e68ea9f8-9b61-414e-8885-3033b67c2850"
   },
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade transformers accelerate datasets hf_transfer -q\n",
    "# !pip3 install librosa tensorboardX -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPy5dKOoaMEF"
   },
   "outputs": [],
   "source": [
    "# For using wandb\n",
    "\n",
    "# !pip3 install wandb -q\n",
    "# !wandb login $WB_TOKEN\n",
    "\n",
    "# wandb settings\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"Qwen-German\"\n",
    "# os.environ[\"WANDB_LOG_MODEL\"] = \"end\" # or \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nyLBwrjr_Mu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_lang_code = \"de\"\n",
    "# tgt_lang_code = \"zh\"\n",
    "# tgt_lang_code = \"ar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dYWcf2Ap0imm"
   },
   "outputs": [],
   "source": [
    "data_cache_dir = \"/workspace/data/\"\n",
    "model_cache_dir = \"/workspace/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVWtSqIcZC4j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'audio', 'text_en', 'text_ar', 'text_de', 'text_fa', 'text_fr', 'text_ja', 'text_nl', 'text_pt', 'text_ru', 'text_tr', 'text_zh'],\n",
       "        num_rows: 784\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'audio', 'text_en', 'text_ar', 'text_de', 'text_fa', 'text_fr', 'text_ja', 'text_nl', 'text_pt', 'text_ru', 'text_tr', 'text_zh'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "cache_dir = \"/workspace/cache\"\n",
    "\n",
    "dataset_name = \"ymoslem/ACL-6060\"\n",
    "\n",
    "dataset = load_dataset(dataset_name,\n",
    "                       split=\"dev+eval\",\n",
    "                       cache_dir=cache_dir\n",
    "                      )\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=100, seed=0)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFGw6SkMaeMr"
   },
   "outputs": [],
   "source": [
    "print(dataset[\"test\"][\"text_en\"][0])\n",
    "print(dataset[\"test\"][f\"text_{tgt_lang_code}\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classifier using BERT or CodeBERT and a generator using BART.\n"
     ]
    }
   ],
   "source": [
    "[\"train\"] = dataset[\"train\"].shuffle(seed=0)\n",
    "\n",
    "print(dataset[\"train\"][\"text_en\"][0])\n",
    "print(dataset[\"train\"][f\"text_{tgt_lang_code}\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current column: {'train': ['audio', 'text_de'], 'test': ['audio', 'text_de']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 784\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.select_columns([\"audio\", f\"text_{tgt_lang_code}\"])\n",
    "print(\"Current column:\", dataset.column_names)\n",
    "\n",
    "dataset = dataset.rename_column(f\"text_{tgt_lang_code}\", \"text\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605"
   },
   "source": [
    "# Load Feature Extractor and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPyujw5bVOzB"
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-Audio-7B-Instruct\"\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "560332eb-3558-41a1-b500-e83a9f695f84"
   },
   "source": [
    "### Load WhisperFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name,\n",
    "                                                         cache_dir=model_cache_dir,\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b"
   },
   "source": [
    "### Load the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name,\n",
    "                                          cache_dir=model_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor.tokenizer.bos_token,\n",
    "      processor.tokenizer.bos_token_id,\n",
    "      processor.tokenizer.eos_token,\n",
    "      processor.tokenizer.eos_token_id,\n",
    "      processor.tokenizer.pad_token_id,\n",
    "      sep=\"\\n\",\n",
    "      end=\"\\n\\n\",\n",
    "     )\n",
    "\n",
    "print(processor.feature_extractor.n_samples)  # 30 seconds\n",
    "print(processor.feature_extractor.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a professional translator.<|im_end|>\n",
      "<|im_start|>user\n",
      "Audio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\n",
      "<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chat template\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# print(processor.tokenizer.chat_template)\n",
    "\n",
    "test_conversation = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are a professional translator.\"},\n",
    "    {\"role\": \"user\", \"content\": \n",
    "     [{\"type\": \"audio\", \"audio\": np.zeros(16000)}]},\n",
    "]\n",
    "print(processor.apply_chat_template(test_conversation, tokenize=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "263a5a58-0239-4a25-b0df-c625fc9c5810"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
   },
   "source": [
    "### Define a Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import numpy as np\n",
    "import librosa\n",
    "import logging\n",
    "import traceback\n",
    "import torch\n",
    "\n",
    "\n",
    "# Set up proper logging instead of print statements\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AudioDataCollator:\n",
    "    \"\"\"\n",
    "    A data collator for processing audio data and preparing it for multimodal models.\n",
    "    Handles audio processing and tokenization with appropriate masking for training.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        processor, \n",
    "        task_prompt: str, \n",
    "        system_prompt: str = \"You are a professional translator.\",\n",
    "        max_length: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the AudioDataCollator.\n",
    "        \n",
    "        Args:\n",
    "            processor: The processor for audio and text inputs\n",
    "            task_prompt: The instruction to include with each audio input\n",
    "            system_prompt: System prompt to include in each conversation\n",
    "            max_length: Maximum audio length (samples). Uses processor default if None\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.task_prompt = task_prompt\n",
    "        self.system_prompt = system_prompt\n",
    "        self.max_length = max_length or processor.feature_extractor.n_samples\n",
    "        self.sampling_rate = processor.feature_extractor.sampling_rate\n",
    "\n",
    "    def process_audio(self, audio: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Process a single audio array.\n",
    "        \n",
    "        Args:\n",
    "            audio: Audio array to process\n",
    "            \n",
    "        Returns:\n",
    "            Processed audio array with correct length and dtype\n",
    "        \"\"\"\n",
    "        if len(audio) > self.max_length:\n",
    "            # Truncate if longer than max_length\n",
    "            audio = audio[:self.max_length]\n",
    "        elif len(audio) < self.max_length:\n",
    "            # Optionally pad if shorter\n",
    "            # audio = np.pad(audio, (0, self.max_length - len(audio)), mode='constant')\n",
    "            pass\n",
    "\n",
    "        return audio.astype(np.float32)\n",
    "\n",
    "    \n",
    "    def find_assistant_token_position(self, input_ids: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Find the position where the assistant's response begins.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Tokenized input sequence\n",
    "            \n",
    "        Returns:\n",
    "            Index of assistant token sequence start position, or -1 if not found\n",
    "        \"\"\"\n",
    "        assistant_start_tokens = self.processor.tokenizer.encode(\n",
    "            \"<|im_start|>assistant\",\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        \n",
    "        # Search for the sequence of tokens in the input\n",
    "        for j in range(len(input_ids) - len(assistant_start_tokens) + 1):\n",
    "            if torch.equal(\n",
    "                input_ids[j:j + len(assistant_start_tokens)],\n",
    "                torch.tensor(assistant_start_tokens)\n",
    "            ):\n",
    "                return j\n",
    "        \n",
    "        return -1\n",
    "    \n",
    "    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Process a batch of examples.\n",
    "        \n",
    "        Args:\n",
    "            examples: List of examples with audio and text fields\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of tensors ready for model input\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If no valid examples can be processed\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        audios = []\n",
    "        valid_examples = []\n",
    "        \n",
    "        # Process each example\n",
    "        for i, example in enumerate(examples):\n",
    "            try:\n",
    "                # Process audio\n",
    "                procesed_audio = self.process_audio(example[\"audio\"][\"array\"])\n",
    "                audios.append(procesed_audio)\n",
    "                \n",
    "                conversation = [\n",
    "                                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                                {\"role\": \"user\", \"content\": [\n",
    "                                    {\"type\": \"audio\", \"audio_url\": example[\"audio\"][\"path\"]},  # for formatting\n",
    "                                    {\"type\": \"text\", \"text\": self.task_prompt},\n",
    "                                ]},\n",
    "                                {\"role\": \"assistant\", \"content\": [\n",
    "                                    {\"type\": \"text\", \"text\": example[\"text\"]},\n",
    "                                ]},\n",
    "                            ]\n",
    "\n",
    "                # Apply chat template\n",
    "                text = self.processor.apply_chat_template(\n",
    "                    conversation,\n",
    "                    add_generation_prompt=False,\n",
    "                    tokenize=False,\n",
    "                )                \n",
    "                texts.append(text)\n",
    "                valid_examples.append(example)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process example {i}: {str(e)}\")\n",
    "                logger.debug(traceback.format_exc())\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Successfully processed {len(valid_examples)} examples out of {len(examples)}\")\n",
    "        \n",
    "        if not valid_examples:\n",
    "            raise ValueError(\"No valid examples found in the batch.\")\n",
    "\n",
    "        try:\n",
    "            # Process batch through processor\n",
    "            inputs = self.processor(\n",
    "                text=texts,\n",
    "                audio=audios,\n",
    "                sampling_rate=self.sampling_rate,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process batch: {str(e)}\")\n",
    "            logger.debug(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "        \n",
    "        # Create labels by cloning input_ids\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        \n",
    "        # Mask the prompt portion (everything before assistant response)\n",
    "        for i in range(len(texts)):\n",
    "            try:\n",
    "                # Find where assistant response starts\n",
    "                assistant_start_idx = self.find_assistant_token_position(inputs[\"input_ids\"][i])\n",
    "                \n",
    "                if assistant_start_idx != -1:\n",
    "                    # Get the length of assistant token sequence\n",
    "                    assistant_tokens_len = len(self.processor.tokenizer.encode(\n",
    "                        \"<|im_start|>assistant\",\n",
    "                        add_special_tokens=False\n",
    "                    ))\n",
    "                    \n",
    "                    # Mask everything before the assistant content\n",
    "                    labels[i, :assistant_start_idx + assistant_tokens_len] = -100\n",
    "                else:\n",
    "                    logger.warning(f\"'<|im_start|>assistant' not found in input for example {i}\")\n",
    "                    labels[i, :] = -100  # Mask everything if token not found\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to mask labels for example {i}: {str(e)}\")\n",
    "                logger.debug(traceback.format_exc())\n",
    "        \n",
    "        # Return formatted batch\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"input_features\": inputs[\"input_features\"],\n",
    "            \"feature_attention_mask\": inputs[\"feature_attention_mask\"],\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import torch\n",
    "\n",
    "def inspect_batch(batch: Dict[str, torch.Tensor], processor) -> None:\n",
    "    \"\"\"Detailed inspection of a batch, including label decoding.\"\"\"\n",
    "    print(\"\\n=== Detailed Batch Inspection ===\")\n",
    "    for key, tensor in batch.items():\n",
    "        print (f\"\\n{key}:\")\n",
    "        print(f\"Shape: {tensor.shape}\")\n",
    "        print(f\"Type: {tensor.dtype}\")\n",
    "        print(f\"Device: {tensor.device}\")\n",
    "        if key not in ['input_features', 'feature_attention_mask']:\n",
    "            print(f\" First row: {tensor[0].tolist()}\")        \n",
    "    \n",
    "    if \"input_ids\" in batch:\n",
    "        # find maximum input length\n",
    "        max_length = batch[\"input_ids\"].shape[1]\n",
    "        print(\"\\n=== Max Input Length ===\")\n",
    "        print(f\"Max input length: {max_length}\")\n",
    "        \n",
    "        # find maximum feature length\n",
    "        max_feature_length = batch[\"input_features\"].shape[1]\n",
    "        print(\"\\n=== Max Feature Length ===\")\n",
    "        print(f\"Max feature length: {max_feature_length}\")\n",
    "        \n",
    "        # Decode input_ids and print the first one\n",
    "        print(\"\\n=== Decoded input_ids ===\")\n",
    "        first_input = batch[\"input_ids\"][0].tolist()\n",
    "        decoded_input = processor.tokenizer.decode(first_input, skip_special_tokens=False)\n",
    "        print(f\"Decoded Input (First Row) :\\n{decoded_input}\")\n",
    "        # Decode labels and print the first one\n",
    "    \n",
    "    if \"labels\" in batch:\n",
    "        print(\"\\n=== Decoded Labels ===\")\n",
    "        first_label = batch[\"labels\"][0].tolist()\n",
    "        # Filter out the misked token (-100) values\n",
    "        valid_tokens = [token for token in first_label if token != -100]\n",
    "        decoded_label = processor.tokenizer.decode(valid_tokens, skip_special_tokens=False)\n",
    "        print(f\"Decoded Label (First Row) :\\n{decoded_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tgt_lang_code == \"de\":\n",
    "    tgt_lang = \"German\"\n",
    "elif tgt_lang_code == \"zh\":\n",
    "    tgt_lang = \"Chinese\"\n",
    "elif tgt_lang_code == \"ar\":\n",
    "    tgt_lang = \"Arabic\"\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported target language code: {tgt_lang_code}\")\n",
    "\n",
    "\n",
    "data_collator = AudioDataCollator(processor=processor, \n",
    "                                  task_prompt=f\"Translate the English speech into {tgt_lang}:\",\n",
    "                                 )\n",
    "\n",
    "dataloader = DataLoader(dataset[\"train\"],\n",
    "                        batch_size=1,  # 1024\n",
    "                        collate_fn=data_collator,\n",
    "                        shuffle=True\n",
    "                       )\n",
    "\n",
    "for batch in dataloader:\n",
    "    inspect_batch(batch, processor)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daf2a825-6d9f-4a23-b145-c37c0039075b"
   },
   "source": [
    "###Â Load a Pre-Trained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f"
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2AudioForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(model_name,\n",
    "                                                           # torch_dtype=torch.bfloat16,  # more efficient\n",
    "                                                           cache_dir=model_cache_dir,\n",
    "                                                           ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pring model number of parameters\n",
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of model parameters: {num_parameters:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.device.type == \"cuda\"  # \"Model must be on GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKdfwbATyUY0"
   },
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original config:\", model.generation_config)\n",
    "model.generation_config.do_sample = False\n",
    "model.generation_config.temperature = None\n",
    "model.generation_config.top_k = None\n",
    "model.generation_config.top_p = None\n",
    "print(\"Modified config:\", model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
   },
   "source": [
    "### Define the Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcVBfH3J3ezh"
   },
   "outputs": [],
   "source": [
    "# Traing arguments\n",
    "\n",
    "import time\n",
    "\n",
    "batch_size = 4\n",
    "accumulation_steps = 1\n",
    "learning_rate = 1e-5\n",
    "scheduler = \"cosine\"\n",
    "warmup_ratio = 0.0\n",
    "epochs = 3\n",
    "\n",
    "weight_decay = 0.001  # set to 0.0 for pruned models\n",
    "\n",
    "data = \"acl6060\"\n",
    "model_prefix = model_name.split(\"/\")[-1]\n",
    "arguments = f\"{batch_size}bs-1e5lr-{scheduler}-{epochs}epoch\"\n",
    "run_name = f\"{model_prefix}-{arguments}-{data}-new\"\n",
    "\n",
    "user_id = \"ymoslem\"  # change to your user ID\n",
    "output_dir = user_id + \"/\" + run_name\n",
    "\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    remove_unused_columns=False,  # Important\n",
    "    \n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=accumulation_steps,\n",
    "    eval_accumulation_steps=accumulation_steps,\n",
    "    \n",
    "    # gradient_checkpointing=True,  # saves memory, but can be slower\n",
    "    # gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "    \n",
    "    bf16=True,\n",
    "    \n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=scheduler,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    \n",
    "    weight_decay=weight_decay,\n",
    "\n",
    "    num_train_epochs=epochs,\n",
    "    \n",
    "    # eval_strategy=\"epoch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    \n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    save_total_limit=2,\n",
    "    # load_best_model_at_end=True,\n",
    "    \n",
    "    logging_steps=1,\n",
    "    report_to=[\"tensorboard\"],  # or [\"tensorboard\", \"wandb\"]\n",
    "    run_name=run_name,\n",
    "    \n",
    "    push_to_hub=True,\n",
    "    hub_private_repo=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ggD7hBfhxS_W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLIVdlzm8GCN"
   },
   "outputs": [],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ea59ed39-3bc5-4c79-9fbe-41abc33053d6"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2zQwMfEOBJq"
   },
   "outputs": [],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f404cf9-4345-468c-8196-4bd101d9bd51"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "cc572c8a-70c0-4efb-acdb-dcf201a3b0dc"
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": [dataset_name],\n",
    "    \"dataset\": [\"ymoslem/ACL-6060\"],\n",
    "    \"language\": [\"en\", tgt_lang_code],\n",
    "    \"model_name\": f\"Qwen2-Audio-7B EN-{tgt_lang_code.upper()} Speech Translation\",\n",
    "    \"finetuned_from\": model_name,\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7030622-caf7-4039-939b-6195cdaa2585"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.push_to_hub(output_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
