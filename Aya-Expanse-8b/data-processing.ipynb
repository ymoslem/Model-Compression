{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR-EN: https://data.statmt.org/news-commentary/v18/training/news-commentary-v18.ar-en.tsv.gz\n",
    "# CS-DE: https://data.statmt.org/news-commentary/v18/training/news-commentary-v18.cs-de.tsv.gz\n",
    "\n",
    "# Czech-German\n",
    "file_path = \"data/news-commentary-v18.cs-de.tsv\"\n",
    "src_lang = \"cs\"\n",
    "tgt_lang = \"de\"\n",
    "full_src_lang = \"czech\"\n",
    "full_tgt_lang = \"german\"\n",
    "\n",
    "# Arabic-English\n",
    "# file_path = \"data/news-commentary-v18.ar-en.tsv\"\n",
    "# src_lang = \"ar\"\n",
    "# tgt_lang = \"en\"\n",
    "# full_src_lang = \"arabic\"\n",
    "# full_tgt_lang = \"english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e33b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pl.read_csv(\n",
    "    file_path,\n",
    "    separator=\"\\t\",\n",
    "    has_header=False,\n",
    "    new_columns=[\"src\", \"tgt\"],\n",
    "    quote_char=None,  # Disable quote parsing to handle unescaped quotes\n",
    "    null_values=[\" \", \"null\"]\n",
    ")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing options\n",
    "\n",
    "src_col = \"src\"\n",
    "tgt_col = \"tgt\"\n",
    "\n",
    "too_long = 200\n",
    "len_ratio = 1.5\n",
    "too_short = 5\n",
    "too_short_type = \"char\" # \"word\" or \"char\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing function\n",
    "\n",
    "def process_dataset_df(df, src_col, tgt_col, len_ratio, too_long, too_short, too_short_type, remove_html=True):\n",
    "\n",
    "    print(\"Dataset original size: \\t-->\", df.shape[0])\n",
    "\n",
    "    df = df.unique()\n",
    "    print(\"Duplicates removed \\t-->\", df.shape[0]) \n",
    "    \n",
    "\n",
    "    df = df.drop_nulls()\n",
    "    print(\"Nulls removed \\t\\t-->\", df.shape[0])\n",
    "\n",
    "    if remove_html:\n",
    "        df = df.with_columns(\n",
    "                        pl.col(src_col)\n",
    "                        .str.replace_all(r'<.*?>|&lt;.*?&gt;|&?(amp|nbsp|quot);|\\{\\}', ' ')\n",
    "                        .str.replace_all(r'\\s+', ' ')\n",
    "                        .str.strip_chars(\" \\t\\n\"),\n",
    "\n",
    "                        pl.col(tgt_col)\n",
    "                        .str.replace_all(r'<.*?>|&lt;.*?&gt;|&?(amp|nbsp|quot);|\\{\\}', ' ')\n",
    "                        .str.replace_all(r'\\s+', ' ')\n",
    "                        .str.strip_chars(\" \\t\\n\")\n",
    "                            )\n",
    "        print(\"HTML removed \\t\\t-->\", df.shape[0])\n",
    "\n",
    "    no_list = ['�', '؟؟؟']\n",
    "    df = df.filter(pl.col(src_col).str.contains_any(no_list) == False)\n",
    "    df = df.filter(pl.col(tgt_col).str.contains_any(no_list) == False)\n",
    "    print(\"Removed no-list \\t-->\", df.shape[0])\n",
    "\n",
    "    df = df.filter(pl.col(src_col).str.split(' ').list.len() <= too_long)\n",
    "    df = df.filter(pl.col(tgt_col).str.split(' ').list.len() <= too_long)\n",
    "    print(\"Too long removed \\t-->\", df.shape[0])\n",
    "\n",
    "    df = df.filter(pl.col(src_col).str.len_chars() <= (pl.col(tgt_col).str.len_chars()) * len_ratio)\n",
    "    df = df.filter(pl.col(tgt_col).str.len_chars() <= (pl.col(src_col).str.len_chars()) * len_ratio)\n",
    "    print(\"Too long ratio removed \\t-->\", df.shape[0])\n",
    "\n",
    "    if too_short_type == \"word\":\n",
    "        df = df.filter(pl.col(src_col).str.split(' ').list.len() > too_short)\n",
    "        df = df.filter(pl.col(tgt_col).str.split(' ').list.len() > too_short)\n",
    "    else:\n",
    "        df = df.filter(pl.col(src_col).str.len_chars() > too_short)\n",
    "        df = df.filter(pl.col(tgt_col).str.len_chars() > too_short)\n",
    "    print(\"Too short removed \\t-->\", df.shape[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a71367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = process_dataset_df(df, src_col, tgt_col, len_ratio, too_long, too_short, too_short_type, remove_html=False)\n",
    "df_filtered.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf15d8",
   "metadata": {},
   "source": [
    "# Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language detection with fasttext\n",
    "# curl  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin -o lid.176.bin\n",
    "\n",
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model(\"lid.176.bin\")\n",
    "\n",
    "def detect_language(model, lines, batch_size):\n",
    "    # Process in batches\n",
    "    results = []\n",
    "    scores = []\n",
    "\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        predictions = model.predict(batch, k=1)\n",
    "        prediction_ids = [pred[0].replace(\"__label__\", \"\") for pred in predictions[0]]\n",
    "        prediction_scores = [pred[0] for pred in predictions[1]]\n",
    "        results.extend(prediction_ids)\n",
    "        scores.extend(prediction_scores)\n",
    "\n",
    "    return results, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb36188",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current dataset size: \", df_filtered.shape[0])\n",
    "\n",
    "# Detect languages for source and target columns\n",
    "src_codes, src_scores = detect_language(model, df_filtered[src_col].to_list(), batch_size=df_filtered.shape[0])\n",
    "tgt_codes, tgt_scores = detect_language(model, df_filtered[tgt_col].to_list(), batch_size=df_filtered.shape[0])\n",
    "\n",
    "df_filtered = df_filtered.with_columns(\n",
    "    pl.Series(\"src_lang\", src_codes),\n",
    "    pl.Series(\"tgt_lang\", tgt_codes),\n",
    "    pl.Series(\"src_score\", src_scores),\n",
    "    pl.Series(\"tgt_score\", tgt_scores)\n",
    ")\n",
    "\n",
    "df_filtered = df_filtered.filter(\n",
    "    (pl.col(\"src_lang\") == src_lang) & (pl.col(\"tgt_lang\") == tgt_lang) & (pl.col(\"src_score\") > 0.9) & (pl.col(\"tgt_score\") > 0.9)\n",
    ")\n",
    "\n",
    "print(\"Language detection completed.\\nCurrent dataset size: \", df_filtered.shape[0])\n",
    "df_filtered.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69765a5",
   "metadata": {},
   "source": [
    "# Semantic filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac053c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "muse_langs = ['ar', 'de', 'en', 'es', 'fr', 'it', 'ko', 'nl', 'pt', 'ru', 'tr', 'zh']\n",
    "para_langs = [\"ar\", \"bg\", \"ca\", \"cs\", \"da\", \"de\", \"en\", \"el\", \"es\", \"et\", \"fa\", \"fi\", \"fr\", \"gl\", \"gu\", \"he\", \"hi\", \"hr\", \"hu\", \"hy\", \"id\", \"it\", \"ja\", \"ka\", \"ko\", \"ku\", \"lt\", \"lv\", \"mk\", \"mn\", \"mr\", \"ms\", \"my\", \"nb\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"sk\", \"sl\", \"sq\", \"sr\", \"sv\", \"th\", \"tr\", \"uk\", \"ur\", \"vi\", \"zh\"]\n",
    "\n",
    "if len(src_lang) > 2 or len(tgt_lang) > 2:\n",
    "    raise SystemExit(\"Please use an ISO 639‑1 language code, e.g. 'en'!\")\n",
    "elif src_lang in muse_langs and tgt_lang in muse_langs:\n",
    "    model_name = \"distiluse-base-multilingual-cased-v1\"  # 15 languages\n",
    "elif src_lang in para_langs and tgt_lang in para_langs:\n",
    "    model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"  # 50 languages\n",
    "else:\n",
    "    raise SystemExit(\"Language pair is not supported!\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "print(\"Model loaded:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8153b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_filter(df, src_col, tgt_col, threshold=0.6, batch_size=32, device=\"cpu\"):\n",
    "    print(\"Current dataset size: \", df.shape[0])\n",
    "    print(\"Semantic filtering started...\")\n",
    "\n",
    "    print(\"Encoding source texts...\")\n",
    "    src_embeddings = model.encode(\n",
    "        df[src_col].to_list(),\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    print(\"Encoding target texts...\")\n",
    "    tgt_embeddings = model.encode(\n",
    "        df[tgt_col].to_list(),\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    cosine_scores = torch.nn.functional.cosine_similarity(src_embeddings, tgt_embeddings)\n",
    "    # Convert tensor to numpy array then to list for Polars\n",
    "    mask = cosine_scores.cpu().numpy() > threshold\n",
    "    df_filtered = df.filter(pl.Series(mask))\n",
    "\n",
    "    print(\"Semantic filtering completed.\\nFiltered dataset size: \", df_filtered.shape[0])\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_semantic_filtered = semantic_filter(df_filtered, src_col, tgt_col, threshold=0.7, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to keep\n",
    "df_semantic_filtered = df_semantic_filtered.select([\"src\", \"tgt\"]).rename({\"src\": full_src_lang, \"tgt\": full_tgt_lang})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only when we need to use a backup ndjson file\n",
    "\n",
    "# # Save the filtered dataset to ndjson\n",
    "# df_semantic_filtered.write_ndjson(\"filtered_dataset.ndjson\")\n",
    "\n",
    "# # Load the filtered dataset from ndjson\n",
    "# import polars as pl\n",
    "\n",
    "# df_semantic_filtered = pl.read_ndjson(\"filtered_dataset.ndjson\")\n",
    "\n",
    "# print(df_semantic_filtered.shape)\n",
    "# df_semantic_filtered.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c229f",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# # convert dataframe to dictionary\n",
    "dataset_dict = df_semantic_filtered.to_dict(as_series=False)\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c539dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = f\"ymoslem/news-commentary-{src_lang}-{tgt_lang}\"\n",
    "print(f\"Pushing dataset to Hugging Face Hub: {output_dataset}\")\n",
    "\n",
    "dataset.push_to_hub(output_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
