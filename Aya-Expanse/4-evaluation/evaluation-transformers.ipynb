{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56Rehreqm7ZA"
   },
   "outputs": [],
   "source": [
    "!pip3 install datasets transformers sacrebleu unbabel-comet polars -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language codes\n",
    "full_src_lang = \"Czech\"\n",
    "full_tgt_lang = \"German\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"ymoslem/news-commentary-cs-de\"  # sentence-level data\n",
    "\n",
    "dataset = load_dataset(dataset_name,\n",
    "                       split=\"train\",\n",
    "                      )\n",
    "\n",
    "dataset = dataset.shuffle(seed=0)\n",
    "\n",
    "# Split dataset into train and test\n",
    "dataset = dataset.train_test_split(test_size=500, seed=0)\n",
    "\n",
    "dataset = dataset[\"test\"]\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences = dataset[\"source\"]\n",
    "prompt = f\"Translate the following text from {full_src_lang} to {full_tgt_lang}:\"\n",
    "prompts = [prompt + \"\\n\" + sent + \"\\n\" for sent in source_sentences]\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = dataset[\"target\"]\n",
    "references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_max_len(sentences):\n",
    "    max_len, longest_idx = max([(len(sent.split()), idx)\n",
    "                                for idx, sent in enumerate(sentences)])\n",
    "                                \n",
    "    max_len = max_len * 2\n",
    "    return max_len, longest_idx\n",
    "\n",
    "max_len, longest_idx = define_max_len(source_sentences)\n",
    "\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tucgZZkggqGO"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "layers = 24  # 16, 20, 24\n",
    "\n",
    "model_id = \"CohereLabs/aya-expanse-8b\"\n",
    "# model_id = f\"ymoslem/wmt25-cs-de-{layers}layers-2e-05-100k-news-commentary-sentences\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             dtype=torch.bfloat16,\n",
    "                                            ).to(device).eval()\n",
    "\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "# model.config.max_position_embeddings = 4096  # just to match our vLLM eval\n",
    "\n",
    "assert model.device.type == \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate in batches\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "print(f\"Translating {len(prompts)} prompts...\")\n",
    "\n",
    "batch_size = dataset.num_rows # 500 or try 256 for low memory\n",
    "translations = []\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    \n",
    "    # Format all messages in the batch\n",
    "    batch_messages = [[{\"role\": \"user\", \"content\": prompt}] for prompt in batch_prompts]\n",
    "    \n",
    "    # Tokenize the entire batch and get attention mask\n",
    "    batch_inputs = tokenizer.apply_chat_template(\n",
    "        batch_messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_dict=True  # This returns both input_ids and attention_mask\n",
    "    )\n",
    "    \n",
    "    input_ids = batch_inputs['input_ids'].to(device)\n",
    "    attention_mask = batch_inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Store original lengths for each sequence in the batch\n",
    "    original_length = input_ids.shape[1]  # All sequences have same length due to padding\n",
    "    \n",
    "    # Generate for the entire batch with attention mask\n",
    "    with torch.no_grad():\n",
    "        gen_tokens = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,  # Pass the attention mask\n",
    "            max_new_tokens=max_len,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    # Decode batch results\n",
    "    for j, tokens in enumerate(gen_tokens):\n",
    "        # Get the length of the original input for this specific sequence\n",
    "        new_tokens = tokens[original_length:]\n",
    "        translation = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "        translations.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Save translations to a file\n",
    "# with open(\"output.txt\", \"w\") as output:\n",
    "#     for sentence in translations:\n",
    "#         output.write(sentence.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release memory\n",
    "import gc\n",
    "model = None\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import CHRF\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "chrf = CHRF(word_order=2)\n",
    "\n",
    "chrf_score = round(chrf.corpus_score(translations, [references]).score, 2)\n",
    "\n",
    "all_scores.append(chrf_score)\n",
    "\n",
    "chrf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# Download and load a COMET model\n",
    "comet_model_names = [\"Unbabel/wmt20-comet-da\", \"Unbabel/wmt22-comet-da\"]\n",
    "\n",
    "for comet_model_name in comet_model_names:\n",
    "\n",
    "    model_path = download_model(comet_model_name)\n",
    "    comet_model = load_from_checkpoint(model_path).to(\"cuda\")\n",
    "\n",
    "    assert comet_model.device.type == \"cuda\"\n",
    "\n",
    "    # Prepare the data\n",
    "    data = []\n",
    "    for src, mt, ref in zip(source_sentences, translations, references):\n",
    "        data.append({\n",
    "            \"src\": src,\n",
    "            \"mt\": mt,\n",
    "            \"ref\": ref\n",
    "        })\n",
    "\n",
    "    # Calculate COMET scores\n",
    "    model_output = comet_model.predict(data, batch_size=8, gpus=1)\n",
    "    comet_scores = model_output.scores\n",
    "    comet_corpus_score = round(model_output.system_score * 100, 2)\n",
    "    all_scores.append(comet_corpus_score)\n",
    "\n",
    "    print(comet_model_name)\n",
    "    print(f\"Corpus COMET score: {comet_corpus_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "print(model_id)\n",
    "\n",
    "df = pl.DataFrame([all_scores],\n",
    "                  schema=[\"chrF++\", \"COMET20\", \"COMET22\"],\n",
    "                  orient=\"row\",\n",
    "                 )\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
