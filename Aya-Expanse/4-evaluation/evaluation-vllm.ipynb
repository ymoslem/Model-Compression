{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56Rehreqm7ZA"
   },
   "outputs": [],
   "source": [
    "# This might need a restart, so better run it in the Terminal first.\n",
    "# !pip3 install vllm datasets sacrebleu unbabel-comet polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language codes\n",
    "full_src_lang = \"Czech\"\n",
    "full_tgt_lang = \"German\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"ymoslem/news-commentary-cs-de\"  # sentence-level data\n",
    "\n",
    "dataset = load_dataset(dataset_name,\n",
    "                       split=\"train\",\n",
    "                      )\n",
    "\n",
    "dataset = dataset.shuffle(seed=0)\n",
    "\n",
    "# Split dataset into train and test\n",
    "dataset = dataset.train_test_split(test_size=500, seed=0)\n",
    "\n",
    "dataset = dataset[\"test\"]\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences = dataset[\"source\"]\n",
    "prompt = f\"Translate the following text from {full_src_lang} to {full_tgt_lang}:\"\n",
    "prompts = [prompt + \"\\n\" + sent + \"\\n\" for sent in source_sentences]\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = dataset[\"target\"]\n",
    "references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_max_len(sentences):\n",
    "    max_len, longest_idx = max([(len(sent.split()), idx)\n",
    "                                for idx, sent in enumerate(sentences)])\n",
    "                                \n",
    "    max_len = max_len * 2\n",
    "    return max_len, longest_idx\n",
    "\n",
    "max_len, longest_idx = define_max_len(source_sentences)\n",
    "\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "layers = 24  # 16, 20, 24\n",
    "\n",
    "model_name = \"CohereLabs/aya-expanse-8b\"\n",
    "# model_name = f\"ymoslem/wmt25-cs-de-{layers}layers-2e-05-100k-news-commentary-sentences\"\n",
    "\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "awq = True if \"-awq\" in model_name.lower() else False  # verify based on your model\n",
    "max_model_len = 4096\n",
    "\n",
    "\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Number of GPUs: {num_gpus}\")\n",
    "print(f\"Max length: {max_len}\")\n",
    "print(f\"AWQ: {awq}\\n\")\n",
    "\n",
    "\n",
    "if awq:\n",
    "    llm = LLM(model=model_name,\n",
    "              #download_dir=model_directory,\n",
    "              trust_remote_code=True,\n",
    "              tensor_parallel_size=num_gpus,\n",
    "              quantization=\"awq_marlin\",\n",
    "              max_model_len=max_model_len,\n",
    "             )\n",
    "else:\n",
    "    llm = LLM(model=model_name,\n",
    "              #download_dir=model_directory,\n",
    "              trust_remote_code=True,\n",
    "              dtype=torch.bfloat16,\n",
    "              tensor_parallel_size=num_gpus,\n",
    "              max_model_len=max_model_len,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"Translating {len(prompts)} prompts...\")\n",
    "\n",
    "# Set up sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "                                temperature=0.0,  # Deterministic generation\n",
    "                                max_tokens=max_len,\n",
    "                                stop_token_ids=[llm.get_tokenizer().eos_token_id],\n",
    "                                )\n",
    "\n",
    "# Format all prompts for chat (if using instruct model)\n",
    "formatted_prompts = []\n",
    "for prompt in tqdm(prompts, desc=\"Formatting prompts\"):\n",
    "    # Format as chat\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = llm.get_tokenizer().apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    formatted_prompts.append(formatted_prompt)\n",
    "\n",
    "# Generate all responses at once (vLLM handles batching internally)\n",
    "print(\"Generating responses...\")\n",
    "batch_outputs = llm.generate(formatted_prompts, sampling_params)\n",
    "\n",
    "# Extract the generated text\n",
    "translations = []\n",
    "for output in batch_outputs:\n",
    "    generated_text = output.outputs[0].text.strip()\n",
    "    translations.append(generated_text)\n",
    "\n",
    "print(f\"Generated {len(translations)} responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: Save the translations to a file\n",
    "# with open(\"output.txt\", \"w\") as output:\n",
    "#     for sentence in translations:\n",
    "#         output.write(sentence.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release memory\n",
    "\n",
    "def release_memory(model):\n",
    "    import gc\n",
    "    model = None\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "release_memory(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import CHRF\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "chrf = CHRF(word_order=2)\n",
    "\n",
    "chrf_score = round(chrf.corpus_score(translations, [references]).score, 2)\n",
    "\n",
    "all_scores.append(chrf_score)\n",
    "\n",
    "chrf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# Download and load a COMET model\n",
    "comet_model_names = [\"Unbabel/wmt20-comet-da\", \"Unbabel/wmt22-comet-da\"]\n",
    "\n",
    "for comet_model_name in comet_model_names:\n",
    "\n",
    "    model_path = download_model(comet_model_name)\n",
    "    comet_model = load_from_checkpoint(model_path).to(\"cuda\")\n",
    "\n",
    "    assert comet_model.device.type == \"cuda\"\n",
    "\n",
    "    # Prepare the data\n",
    "    data = []\n",
    "    for src, mt, ref in zip(source_sentences, translations, references):\n",
    "        data.append({\n",
    "            \"src\": src,\n",
    "            \"mt\": mt,\n",
    "            \"ref\": ref\n",
    "        })\n",
    "\n",
    "    # Calculate COMET scores\n",
    "    model_output = comet_model.predict(data, batch_size=8, gpus=1)\n",
    "    comet_scores = model_output.scores\n",
    "    comet_corpus_score = round(model_output.system_score * 100, 2)\n",
    "    all_scores.append(comet_corpus_score)\n",
    "    \n",
    "    release_memory(comet_model)\n",
    "\n",
    "    print(comet_model_name)\n",
    "    print(f\"Corpus COMET score: {comet_corpus_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "df = pl.DataFrame([all_scores],\n",
    "                  schema=[\"chrF++\", \"COMET20\", \"COMET22\"],\n",
    "                  orient=\"row\",\n",
    "                 )\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
